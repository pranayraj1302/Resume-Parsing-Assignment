{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk spacy scikit-learn sentence-transformers python-docx PyMuPDF --quiet"
      ],
      "metadata": {
        "id": "TlEdbN-PdLe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import spacy\n",
        "spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VP-ibEgdOnv",
        "outputId": "640ff429-6e58-40d8-b8ec-885f48ec1546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7e44662bb1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP & ML\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# PDF & DOCX\n",
        "import fitz  # PyMuPDF\n",
        "from docx import Document\n",
        "\n",
        "# Utility\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import os"
      ],
      "metadata": {
        "id": "yBdzjNWNdbhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(file_path):\n",
        "    text = \"\"\n",
        "    with fitz.open(file_path) as doc:\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "    return text"
      ],
      "metadata": {
        "id": "zev78TsydkJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    return text"
      ],
      "metadata": {
        "id": "osxdQXvwdqF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_resume_text(file_path):\n",
        "    if file_path.endswith('.pdf'):\n",
        "        return extract_text_from_pdf(file_path)\n",
        "    elif file_path.endswith('.docx'):\n",
        "        return extract_text_from_docx(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type. Please use .pdf or .docx\")"
      ],
      "metadata": {
        "id": "i9Yx-VRndsbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "To813cyEdu5f",
        "outputId": "97eaeba5-8729-496c-d68f-3f5b48278eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7e03771e-bb47-463b-a95b-261758bab867\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7e03771e-bb47-463b-a95b-261758bab867\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving CV update_Test.pdf to CV update_Test.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_text = extract_resume_text(\"CV update_Test.pdf\")\n",
        "print(resume_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJUPJduTeW43",
        "outputId": "a08b2103-b658-4cd1-a9db-dc8de46fe0a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lydia Sharon James \n",
            "E-Mail : lydiasharon2907@gmail.com \n",
            "Contact : 8297995761 \n",
            "============================================================================ \n",
            "AI and ML specialist with a Master's degree from IIIT Bangalore, proficient in developing innovative \n",
            "solutions and deriving actionable insights. Passionate about optimizing communication and solving challenges \n",
            "using AI and ML techniques. Skilled in data analysis, predictive modeling, and statistical methodologies, \n",
            "committed to continual advancement and exceeding performance benchmarks in the dynamic field of data \n",
            "science.\n",
            "Experience \n",
            "Infosys Private Limited, Hyderabad \n",
            "Systems Engineer \n",
            "Dec 2021 – Present\n",
            "●\n",
            "Proficient in Dataiku and MicroStrategy, optimizing data workflows and enhancing data quality \n",
            "through advanced data modeling, warehousing, and building robust ETL pipelines.\n",
            "●\n",
            "Spearheaded data engineering and prompt engineering solutions as part of the \"Ask Ralph\" initiative \n",
            "for Ralph Lauren, streamlining reporting and providing actionable data insights.\n",
            "●\n",
            "Developed and managed scalable data pipelines, utilizing SQL, Python, and Spark to support \n",
            "automated reporting and ensure efficient data integration across large-scale systems.\n",
            "●\n",
            "Implemented data structures following best practices in data modeling, ETL/ELT processes, and \n",
            "leveraging tools like Redshift for enterprise-level data management.\n",
            "●\n",
            "Collaborated with cross-functional teams, including data scientists, product managers, software \n",
            "engineers, and operations specialists, to transform raw data into actionable insights, empowering data-\n",
            "driven decision-making.\n",
            "●\n",
            "Demonstrated strong analytical capabilities to improve strategic reporting processes, enhancing \n",
            "business intelligence outcomes.\n",
            "●\n",
            "Innovated and automated ongoing reporting and analysis workflows, ensuring seamless delivery of \n",
            "high-quality data for business analysts and data scientists.\n",
            "LetsEndorse, Bengaluru \n",
            "Intern \n",
            "Oct 2020 - Nov 2020\n",
            "●\n",
            "Worked as an Outreach Intern for the Mission Swavalamban project, focusing on market research, \n",
            "communications, and outreach.\n",
            "●\n",
            "Exhibited independence and a willingness to learn while delivering on assigned tasks.\n",
            "2\n",
            "Projects: \n",
            "Automatic Ticket Classification \n",
            "Indian Institute of Information Technology, Bangalore \n",
            "Sept 2023\n",
            "●\n",
            "Developed a project to organize customer complaints based on products/services involved.\n",
            "●\n",
            "Employed non-negative matrix factorization (NMF) to analyze common words and patterns.\n",
            "●\n",
            "Grouped complaints into categories for efficient handling, improving customer satisfaction.\n",
            "Gesture Recognition \n",
            "Indian Institute of Information Technology \n",
            "June 2022\n",
            "●\n",
            "Led recognition efforts at Photon, identifying promising startups through market research.\n",
            "●\n",
            "Collaborated to assess feasibility and alignment with company goals.\n",
            "Cryptography: Types and History \n",
            "St. Pious X Degree & PG College For Women \n",
            "June 2020\n",
            "●\n",
            "Studied the history and types of cryptography, focusing on its application across various fields.\n",
            "Skills\n",
            "●\n",
            "Data Tools: Informatica Developer, Dataiku, MicroStrategy\n",
            "●\n",
            "Languages: Python, C, Java, SQL, PL/SQL, C++, HTML, JavaScript\n",
            "●\n",
            "Databases: Oracle, MS SQL Server\n",
            "●\n",
            "Machine Learning & AI: Proficient in predictive modeling and statistical analysis\n",
            "●\n",
            "Software: Adobe Photoshop, Adobe Premiere Pro\n",
            "●\n",
            "Methodologies: Agile (SCRUM)\n",
            "Strengths\n",
            "●\n",
            "Self-motivated with the ability to work collaboratively within teams.\n",
            "●\n",
            "Capable of handling pressure with a positive attitude.\n",
            "●\n",
            "Knowledge in digital design (banners, posters, presentations).\n",
            "●\n",
            "Engaged in all-round development through sports and extracurricular activities.\n",
            "●\n",
            "Good communication skills in English, Hindi, and Telugu.\n",
            "3\n",
            "Bio-Data \n",
            "Name\t \t\n",
            "\t\n",
            "\t\n",
            "\t\n",
            "\t\n",
            "\t\n",
            "- Lydia Sharon James\t  \n",
            "Father ’s Name                                                         - Nigel Casper James \n",
            "Date Of Birth                                                            -  29/07/2000 \n",
            "I hear by acknowledge that the above details are fully authentic to the best of my knowledge.  \n",
            "\t\n",
            "\t\n",
            "\t\n",
            "\t\n",
            "\t\n",
            "\t\n",
            "\t\n",
            "\t\n",
            "\t\n",
            "\t\n",
            "\t\n",
            "Lydia Sharon James \n",
            "Education\n",
            "Name of the institute\n",
            "Course of Study \n",
            "Year of Passing\n",
            "Percentage secured\n",
            "Indian Institute of Information \n",
            "Technology\t\n",
            "[Bangalore] \n",
            "Masters in Artificial \n",
            "Intelligence and Machine \n",
            "Learning\n",
            "2024\n",
            "90%\n",
            "St. Pious X Degree & PG \n",
            "College, Hyderabad  \n",
            "Bachelors of Computer Science\n",
            "2021\n",
            "90%\n",
            "Little Flower Junior College, \n",
            "Hyderabad    \n",
            "Inter I and IIyr\n",
            "2018\n",
            "70%\n",
            "St. Ann’s Grammar School, \n",
            "Hyderabad \n",
            "10th grade\n",
            "2016 \n",
            "85%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Cleaning**"
      ],
      "metadata": {
        "id": "OVl07M2BevFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)  # remove extra spaces, tabs, newlines\n",
        "    text = text.strip()\n",
        "    return text.lower()"
      ],
      "metadata": {
        "id": "kDDow34Ie3ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_resume_text = clean_text(resume_text)\n",
        "print(cleaned_resume_text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_VZIIzme7BZ",
        "outputId": "be4cdfdc-8fc4-4bb9-dfcc-a1d2a9ca2e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lydia sharon james e-mail : lydiasharon2907@gmail.com contact : 8297995761 ============================================================================ ai and ml specialist with a master's degree from iiit bangalore, proficient in developing innovative solutions and deriving actionable insights. passionate about optimizing communication and solving challenges using ai and ml techniques. skilled in data analysis, predictive modeling, and statistical methodologies, committed to continual advanceme\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extract Skills**"
      ],
      "metadata": {
        "id": "Qr-wIz3QfEJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skill_keywords = [\n",
        "    'python', 'java', 'javascript', 'typescript', 'kotlin', 'c++', 'c#', 'go', 'ruby', 'php',\n",
        "    'html', 'css', 'react', 'angular', 'vue', 'node.js', 'express', 'django', 'flask', 'spring boot',\n",
        "    'git', 'github', 'bitbucket', 'docker', 'kubernetes', 'aws', 'azure', 'gcp', 'ec2', 's3',\n",
        "    'mysql', 'postgresql', 'sqlite', 'mongodb', 'redis', 'elasticsearch',\n",
        "    'tensorflow', 'keras', 'pytorch', 'numpy', 'pandas', 'scikit-learn', 'matplotlib', 'seaborn',\n",
        "    'jira', 'confluence', 'agile', 'scrum', 'rest api', 'graphql', 'ci/cd', 'linux', 'bash', 'mlops'\n",
        "]"
      ],
      "metadata": {
        "id": "Xjj4YPNcfKl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_skills(text):\n",
        "    found_skills = []\n",
        "    for skill in skill_keywords:\n",
        "        if skill in text:\n",
        "            found_skills.append(skill)\n",
        "    return list(set(found_skills))"
      ],
      "metadata": {
        "id": "khojCEVTfOty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skills = extract_skills(cleaned_resume_text)\n",
        "print(\"Skills:\", skills)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKoQvMlKfRpH",
        "outputId": "a9e5f157-804e-4595-c905-749367ddeebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skills: ['python', 'go', 'scrum', 'agile', 'javascript', 'html', 'git', 'java', 'c++']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extract Education Details**"
      ],
      "metadata": {
        "id": "xrpw-4Apf550"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Define a list of common education-related keywords\n",
        "education_keywords = ['b.tech', 'bsc', 'msc', 'mca', 'mba', 'bachelor', 'degree', 'master', 'university', 'institute', 'college']\n",
        "\n",
        "def extract_education(text):\n",
        "    # Find all occurrences of education-related keywords and extract surrounding details\n",
        "    education_info = []\n",
        "    for keyword in education_keywords:\n",
        "        if keyword in text:\n",
        "            # Find text related to the keyword, and assume it's the education information\n",
        "            matches = re.findall(r'([A-Za-z0-9\\s,\\.]+(?:' + re.escape(keyword) + r'[A-Za-z0-9\\s,\\.]*)+)', text)\n",
        "            education_info.extend(matches)\n",
        "    return education_info"
      ],
      "metadata": {
        "id": "pV_xCazbf9GX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "education_details = extract_education(cleaned_resume_text)\n",
        "print(\"Education:\", education_details)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibe3NfQOgLF_",
        "outputId": "e236fefd-5ce5-46bb-b152-e21a40480696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Education: [' worked as an outreach intern for the mission swavalamban project, focusing on market research, communications, and outreach. ', ' pg college, hyderabad bachelors of computer science 2021 90', 's degree from iiit bangalore, proficient in developing innovative solutions and deriving actionable insights. passionate about optimizing communication and solving challenges using ai and ml techniques. skilled in data analysis, predictive modeling, and statistical methodologies, committed to continual advancement and exceeding performance benchmarks in the dynamic field of data science. experience infosys private limited, hyderabad systems engineer dec 2021 ', ' types and history st. pious x degree ', ' st. pious x degree ', ' ai and ml specialist with a master', ' masters in artificial intelligence and machine learning 2024 90', ' automatic ticket classification indian institute of information technology, bangalore sept 2023 ', ' grouped complaints into categories for efficient handling, improving customer satisfaction. gesture recognition indian institute of information technology june 2022 ', '2000 i hear by acknowledge that the above details are fully authentic to the best of my knowledge. lydia sharon james education name of the institute course of study year of passing percentage secured indian institute of information technology ', ' pg college for women june 2020 ', ' pg college, hyderabad bachelors of computer science 2021 90', ' little flower junior college, hyderabad inter i and iiyr 2018 70']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extract Work Experience**"
      ],
      "metadata": {
        "id": "6MRKVvAugZL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_experience(text):\n",
        "    # Define common work-related keywords\n",
        "    experience_keywords = ['experience', 'worked as', 'role', 'intern', 'company', 'position', 'responsibilities']\n",
        "\n",
        "    experience_info = []\n",
        "    for keyword in experience_keywords:\n",
        "        if keyword in text:\n",
        "            # Find the surrounding text for job-related keywords\n",
        "            matches = re.findall(r'([A-Za-z0-9\\s,\\.]+(?:' + re.escape(keyword) + r'[A-Za-z0-9\\s,\\.]*)+)', text)\n",
        "            experience_info.extend(matches)\n",
        "    return experience_info\n"
      ],
      "metadata": {
        "id": "Z6IbGeirgcHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experience_details = extract_experience(cleaned_resume_text)\n",
        "print(\"Work Experience:\", experience_details)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMFvfwG4ghBW",
        "outputId": "a964064e-d953-4a1f-88af-f7af9b35d3f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Work Experience: ['s degree from iiit bangalore, proficient in developing innovative solutions and deriving actionable insights. passionate about optimizing communication and solving challenges using ai and ml techniques. skilled in data analysis, predictive modeling, and statistical methodologies, committed to continual advancement and exceeding performance benchmarks in the dynamic field of data science. experience infosys private limited, hyderabad systems engineer dec 2021 ', ' worked as an outreach intern for the mission swavalamban project, focusing on market research, communications, and outreach. ', 'quality data for business analysts and data scientists. letsendorse, bengaluru intern oct 2020 ', ' worked as an outreach intern for the mission swavalamban project, focusing on market research, communications, and outreach. ', ' collaborated to assess feasibility and alignment with company goals. cryptography']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extract Projects**"
      ],
      "metadata": {
        "id": "CunypzkIgsxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_projects(text):\n",
        "    # Define common project-related keywords\n",
        "    project_keywords = ['project', 'academic project', 'personal project', 'worked on', 'built', 'developed']\n",
        "\n",
        "    project_info = []\n",
        "    for keyword in project_keywords:\n",
        "        if keyword in text:\n",
        "            # Find the surrounding text for project-related keywords\n",
        "            matches = re.findall(r'([A-Za-z0-9\\s,\\.]+(?:' + re.escape(keyword) + r'[A-Za-z0-9\\s,\\.]*)+)', text)\n",
        "            project_info.extend(matches)\n",
        "    return project_info"
      ],
      "metadata": {
        "id": "4kc39EcCguan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_details = extract_projects(cleaned_resume_text)\n",
        "print(\"Projects:\", project_details)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmN-OobMgy_e",
        "outputId": "ac952fd4-79b7-4266-ed43-4a0eb749c065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projects: [' worked as an outreach intern for the mission swavalamban project, focusing on market research, communications, and outreach. ', ' exhibited independence and a willingness to learn while delivering on assigned tasks. 2 projects', ' developed a project to organize customer complaints based on products', ' developed and managed scalable data pipelines, utilizing sql, python, and spark to support automated reporting and ensure efficient data integration across large', ' developed a project to organize customer complaints based on products']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Match Resume with Job Description**"
      ],
      "metadata": {
        "id": "cf0JlWFThACZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhNmCjwfhDga",
        "outputId": "f9fb69bc-3079-4f0a-9081-8f93be1d083a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the pre-trained model from Sentence-Transformers\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Function to get embeddings for text\n",
        "def get_embeddings(text):\n",
        "    return model.encode([text])[0]  # Get the embedding for the text\n",
        "\n",
        "# Example resume and job description (replace these with your actual data)\n",
        "resume_text = cleaned_resume_text\n",
        "job_description_text = \"\"\"\n",
        "We are looking for a Backend Developer with strong experience in Python, Django, AWS, and MySQL.\n",
        "You should have a background in building scalable applications, working with APIs, and deploying services on the cloud.\n",
        "\"\"\"\n",
        "\n",
        "# Get embeddings for both the resume and the job description\n",
        "resume_embedding = get_embeddings(resume_text)\n",
        "job_description_embedding = get_embeddings(job_description_text)\n",
        "\n",
        "# Calculate cosine similarity between the resume and job description\n",
        "similarity_score = cosine_similarity([resume_embedding], [job_description_embedding])[0][0]\n",
        "\n",
        "# Print the similarity score\n",
        "print(f\"Cosine Similarity between resume and job description: {similarity_score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC4wlmz3hJo3",
        "outputId": "746e1122-fbb6-4708-ac51-72b618ab96c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity between resume and job description: 0.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation**"
      ],
      "metadata": {
        "id": "EGE41ZrCiBZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Create the Dataset***"
      ],
      "metadata": {
        "id": "ZiGy1aY6jCUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample dataset of resumes and job titles\n",
        "data = {\n",
        "    'Resume': [\n",
        "        \"Experienced Python developer with strong skills in web development using Django, Flask, and FastAPI. Hands-on experience with RESTful API design, Docker, PostgreSQL, and AWS services. Worked on several projects in machine learning using TensorFlow and Scikit-learn.\",\n",
        "        \"Skilled Backend Developer specializing in Java, Spring Boot, and Hibernate. Experience with building scalable applications, using Kafka, RabbitMQ for messaging, and deploying applications on AWS. Familiar with SQL and NoSQL databases.\",\n",
        "        \"Highly motivated front-end developer with expertise in JavaScript, React, HTML, CSS, and Vue.js. Experience in building responsive UIs and integrating RESTful APIs. Knowledge of state management using Redux and MobX.\",\n",
        "        \"Data Scientist with a passion for machine learning and artificial intelligence. Experienced with Python libraries such as Pandas, NumPy, Scikit-learn, TensorFlow, and PyTorch. Worked on projects involving predictive modeling, data analysis, and recommendation systems.\",\n",
        "        \"Full Stack Developer with experience in both front-end and back-end technologies. Proficient in JavaScript (React, Node.js), SQL databases, and RESTful APIs. Worked on large-scale web applications and microservices architecture.\",\n",
        "        \"A skilled DevOps Engineer with hands-on experience in setting up CI/CD pipelines, Docker, Kubernetes, AWS, and Jenkins. Automated infrastructure and deployment processes, improving overall efficiency.\",\n",
        "        \"Experienced Project Manager with knowledge in Agile and Scrum methodologies. Worked with cross-functional teams to deliver projects on time, managing the project lifecycle, including requirements gathering, planning, execution, and monitoring.\",\n",
        "        \"Quality Assurance Engineer with expertise in manual and automated testing. Proficient in using Selenium, JUnit, and TestNG for test automation. Experience in working in Agile teams and ensuring quality standards.\",\n",
        "        \"Cybersecurity Specialist with expertise in threat analysis, network security, and vulnerability management. Skilled in using tools like Wireshark, Nessus, and Metasploit to secure systems and perform penetration testing.\",\n",
        "        \"Business Analyst with experience in requirement gathering, process modeling, and data analysis. Skilled in using tools like Microsoft Excel, Tableau, SQL, and Power BI to create insightful reports.\",\n",
        "        \"Cloud Engineer with expertise in cloud platforms like AWS, Azure, and Google Cloud. Experience in building and managing cloud infrastructure, automation of cloud resources, and optimizing performance and cost.\",\n",
        "        \"Mobile App Developer with proficiency in building Android and iOS applications. Strong in Java, Kotlin, Swift, and React Native. Experience in working with Firebase, SQLite, and implementing app functionality.\",\n",
        "        \"Network Engineer with experience in designing, implementing, and managing networks. Expertise in routing protocols, firewall configurations, VPNs, and network monitoring tools like Wireshark and SolarWinds.\",\n",
        "        \"Machine Learning Engineer with experience in developing, deploying, and maintaining machine learning models. Skilled in Python, TensorFlow, Keras, PyTorch, and NLP techniques for text analysis.\",\n",
        "        \"UI/UX Designer with expertise in creating user-centered designs for mobile and web apps. Proficient in using design tools such as Figma, Sketch, Adobe XD, and conducting user research and usability testing.\",\n",
        "        \"Salesforce Developer with strong knowledge of Apex, Visualforce, Lightning components, and Salesforce APIs. Experience in building and deploying custom Salesforce applications and integrations.\",\n",
        "        \"Systems Analyst with experience in analyzing and designing software solutions. Proficient in working with business stakeholders, creating requirements documents, and translating them into technical specifications.\",\n",
        "        \"Embedded Systems Engineer with experience in designing and developing embedded software for microcontrollers and real-time systems. Skilled in C, C++, and Python programming languages, and familiar with IoT development.\",\n",
        "        \"Operations Manager with experience in overseeing business operations and ensuring efficient workflow. Expertise in process optimization, logistics, and managing cross-functional teams.\",\n",
        "        \"Digital Marketing Specialist with expertise in SEO, SEM, content marketing, and social media strategies. Proficient in using tools like Google Analytics, SEMrush, and Hootsuite to enhance brand visibility.\",\n",
        "        \"HR Manager with experience in recruitment, performance management, and employee relations. Skilled in using HR software like BambooHR, Workday, and in managing the full employee lifecycle.\"\n",
        "    ],\n",
        "    'Job_Role': [\n",
        "        'Software Engineer',\n",
        "        'Backend Developer',\n",
        "        'Frontend Developer',\n",
        "        'Data Scientist',\n",
        "        'Full Stack Developer',\n",
        "        'DevOps Engineer',\n",
        "        'Project Manager',\n",
        "        'QA Engineer',\n",
        "        'Cybersecurity Specialist',\n",
        "        'Business Analyst',\n",
        "        'Cloud Engineer',\n",
        "        'Mobile App Developer',\n",
        "        'Network Engineer',\n",
        "        'Machine Learning Engineer',\n",
        "        'UI/UX Designer',\n",
        "        'Salesforce Developer',\n",
        "        'Systems Analyst',\n",
        "        'Embedded Systems Engineer',\n",
        "        'Operations Manager',\n",
        "        'Digital Marketing Specialist',\n",
        "        'HR Manager'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Show the dataset\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Bn3Eq6DiE7_",
        "outputId": "5f7e60e4-c960-4877-a509-2314578e9d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Resume  \\\n",
            "0   Experienced Python developer with strong skill...   \n",
            "1   Skilled Backend Developer specializing in Java...   \n",
            "2   Highly motivated front-end developer with expe...   \n",
            "3   Data Scientist with a passion for machine lear...   \n",
            "4   Full Stack Developer with experience in both f...   \n",
            "5   A skilled DevOps Engineer with hands-on experi...   \n",
            "6   Experienced Project Manager with knowledge in ...   \n",
            "7   Quality Assurance Engineer with expertise in m...   \n",
            "8   Cybersecurity Specialist with expertise in thr...   \n",
            "9   Business Analyst with experience in requiremen...   \n",
            "10  Cloud Engineer with expertise in cloud platfor...   \n",
            "11  Mobile App Developer with proficiency in build...   \n",
            "12  Network Engineer with experience in designing,...   \n",
            "13  Machine Learning Engineer with experience in d...   \n",
            "14  UI/UX Designer with expertise in creating user...   \n",
            "15  Salesforce Developer with strong knowledge of ...   \n",
            "16  Systems Analyst with experience in analyzing a...   \n",
            "17  Embedded Systems Engineer with experience in d...   \n",
            "18  Operations Manager with experience in overseei...   \n",
            "19  Digital Marketing Specialist with expertise in...   \n",
            "20  HR Manager with experience in recruitment, per...   \n",
            "\n",
            "                        Job_Role  \n",
            "0              Software Engineer  \n",
            "1              Backend Developer  \n",
            "2             Frontend Developer  \n",
            "3                 Data Scientist  \n",
            "4           Full Stack Developer  \n",
            "5                DevOps Engineer  \n",
            "6                Project Manager  \n",
            "7                    QA Engineer  \n",
            "8       Cybersecurity Specialist  \n",
            "9               Business Analyst  \n",
            "10                Cloud Engineer  \n",
            "11          Mobile App Developer  \n",
            "12              Network Engineer  \n",
            "13     Machine Learning Engineer  \n",
            "14                UI/UX Designer  \n",
            "15          Salesforce Developer  \n",
            "16               Systems Analyst  \n",
            "17     Embedded Systems Engineer  \n",
            "18            Operations Manager  \n",
            "19  Digital Marketing Specialist  \n",
            "20                    HR Manager  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Get the Cleaned Resume***"
      ],
      "metadata": {
        "id": "M14GxHHKjNOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV data into a DataFrame\n",
        "df = pd.read_csv('UpdatedResumeDataSet.csv')\n",
        "\n",
        "# Show the DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9I6tcrYi0i_",
        "outputId": "2fde0214-bdbb-42e5-81a1-8f1f133e3712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Category                                             Resume\n",
            "0    Data Science  Skills * Programming Languages: Python (pandas...\n",
            "1    Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...\n",
            "2    Data Science  Areas of Interest Deep Learning, Control Syste...\n",
            "3    Data Science  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...\n",
            "4    Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab...\n",
            "..            ...                                                ...\n",
            "957       Testing  Computer Skills: â¢ Proficient in MS office (...\n",
            "958       Testing  â Willingness to accept the challenges. â ...\n",
            "959       Testing  PERSONAL SKILLS â¢ Quick learner, â¢ Eagerne...\n",
            "960       Testing  COMPUTER SKILLS & SOFTWARE KNOWLEDGE MS-Power ...\n",
            "961       Testing  Skill Set OS Windows XP/7/8/8.1/10 Database MY...\n",
            "\n",
            "[962 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Train a Classification Model***"
      ],
      "metadata": {
        "id": "nCQK5bP4jdEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming you've already loaded the CSV data into the DataFrame 'df'\n",
        "\n",
        "# Drop rows where 'Category' or 'Resume' is missing\n",
        "df = df.dropna(subset=['Category', 'Resume'])\n",
        "\n",
        "# Clean the Resume text (for simplicity, convert to lowercase)\n",
        "df['Cleaned_Resume'] = df['Resume'].apply(lambda x: x.lower())\n",
        "\n",
        "# Map job titles to numeric labels for classification (update as per your job roles)\n",
        "job_title_map = {\n",
        "    'Software Engineer': 0,\n",
        "    'Backend Developer': 1,\n",
        "    'Frontend Developer': 2,\n",
        "    'Data Scientist': 3,\n",
        "    'Full Stack Developer': 4,\n",
        "    'DevOps Engineer': 5,\n",
        "    'Project Manager': 6,\n",
        "    'QA Engineer': 7,\n",
        "    'Cybersecurity Specialist': 8,\n",
        "    'Business Analyst': 9,\n",
        "    'Cloud Engineer': 10,\n",
        "    'Mobile App Developer': 11,\n",
        "    'Network Engineer': 12,\n",
        "    'Machine Learning Engineer': 13,\n",
        "    'UI/UX Designer': 14,\n",
        "    'Salesforce Developer': 15,\n",
        "    'Systems Analyst': 16,\n",
        "    'Embedded Systems Engineer': 17,\n",
        "    'Operations Manager': 18,\n",
        "    'Digital Marketing Specialist': 19,\n",
        "    'HR Manager': 20\n",
        "}\n",
        "\n",
        "# Map job titles to numeric labels\n",
        "df['Job_Label'] = df['Category'].map(job_title_map)\n",
        "\n",
        "# Drop any rows where the mapping failed (i.e., job title not found in the map)\n",
        "df = df.dropna(subset=['Job_Label'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = df['Cleaned_Resume']\n",
        "y = df['Job_Label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcdaYLw7jfYD",
        "outputId": "69543027-d6ca-4eac-dba4-da1f69b8b3b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         5.0       1.00      1.00      1.00        15\n",
            "         9.0       1.00      1.00      1.00        10\n",
            "        18.0       1.00      1.00      1.00        12\n",
            "\n",
            "    accuracy                           1.00        37\n",
            "   macro avg       1.00      1.00      1.00        37\n",
            "weighted avg       1.00      1.00      1.00        37\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbw7rAr5nQan",
        "outputId": "314f5962-4a9d-47e1-caa7-b46c2c3ee7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Category', 'Resume', 'Cleaned_Resume', 'Job_Label'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsGkmkXtk5Qn",
        "outputId": "f53cc0fd-7968-42cc-dc40-fa4bd4b881b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Category                                             Resume  \\\n",
            "403  Business Analyst  Education Details \\r\\n BE Computer Science Mum...   \n",
            "404  Business Analyst  Technical Skills Application Servers: IIS 6.0,...   \n",
            "405  Business Analyst  Key Skills - Requirement Gathering - Requireme...   \n",
            "406  Business Analyst  IT Skills: Area Exposure Modeling Tool: Bizagi...   \n",
            "407  Business Analyst  TECHNOLOGICAL SKILLS â¦ Knowledge of Computer...   \n",
            "\n",
            "                                        Cleaned_Resume  Job_Label  \n",
            "403  education details \\r\\n be computer science mum...        9.0  \n",
            "404  technical skills application servers: iis 6.0,...        9.0  \n",
            "405  key skills - requirement gathering - requireme...        9.0  \n",
            "406  it skills: area exposure modeling tool: bizagi...        9.0  \n",
            "407  technological skills â¦ knowledge of computer...        9.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape and sparsity of the TF-IDF matrix\n",
        "print(\"TF-IDF Train Matrix Shape:\", X_train_tfidf.shape)\n",
        "print(\"Non-zero elements in TF-IDF Train Matrix:\", X_train_tfidf.nnz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk-RwLjPlFOw",
        "outputId": "272b4a61-a906-49f5-e49b-8e9fc05278ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Train Matrix Shape: (86, 2693)\n",
            "Non-zero elements in TF-IDF Train Matrix: 28986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jVgLEBZlR2O",
        "outputId": "d5cc0f69-d885-445a-ae58-d6f30050ae23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         5.0       1.00      1.00      1.00        15\n",
            "         9.0       1.00      1.00      1.00        10\n",
            "        18.0       1.00      1.00      1.00        12\n",
            "\n",
            "    accuracy                           1.00        37\n",
            "   macro avg       1.00      1.00      1.00        37\n",
            "weighted avg       1.00      1.00      1.00        37\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Skills, Education, Experience (NER & Keywords)"
      ],
      "metadata": {
        "id": "1A6P_WSOwYgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "\n",
        "# Load the spaCy model once at the top (you only need to do this once)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Let's assume you have a variable called 'resume_text' (your resume content)\n",
        "doc = nlp(resume_text)\n",
        "\n",
        "# Named Entities (Optional: see all entities)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "\n",
        "# Extract skills (custom list)\n",
        "skills_list = ['Python', 'Machine Learning', 'Data Analysis', 'Java', 'SQL', 'Deep Learning']\n",
        "extracted_skills = [token.text for token in doc if token.text in skills_list]\n",
        "print(\"Skills:\", extracted_skills)\n",
        "\n",
        "# Extract education (look for degrees)\n",
        "education_keywords = ['B.Tech', 'Bachelor', 'Master', 'M.Tech', 'PhD', 'Degree']\n",
        "extracted_education = [sent.text.strip() for sent in doc.sents if any(edu in sent.text for edu in education_keywords)]\n",
        "print(\"Education:\", extracted_education)\n",
        "\n",
        "# Extract experience (simple pattern)\n",
        "experience_pattern = re.compile(r'(\\d+)\\s+years? of experience')\n",
        "experience = experience_pattern.findall(resume_text)\n",
        "print(\"Experience (years):\", experience)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pbrXbQPwcjc",
        "outputId": "5e1c37a5-15db-460a-c052-80f862a25262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lydia sharon james e-mail PERSON\n",
            "8297995761 CARDINAL\n",
            "2021 DATE\n",
            "ralph PERSON\n",
            "ralph lauren PERSON\n",
            "bengaluru ORG\n",
            "oct 2020 - nov 2020 DATE\n",
            "2 CARDINAL\n",
            "classification indian institute of information technology ORG\n",
            "sept 2023 DATE\n",
            "recognition indian institute of information technology ORG\n",
            "june 2022 DATE\n",
            "degree & pg college for ORG\n",
            "june 2020 DATE\n",
            "java PERSON\n",
            "c++, html PERSON\n",
            "english LANGUAGE\n",
            "3 CARDINAL\n",
            "sharon james PERSON\n",
            "nigel casper james PERSON\n",
            "birth - 29/07/2000 NORP\n",
            "lydia sharon james PERSON\n",
            "year DATE\n",
            "indian institute of information technology ORG\n",
            "2024 90% PERCENT\n",
            "degree & pg college ORG\n",
            "2021 DATE\n",
            "90% PERCENT\n",
            "iiyr NORP\n",
            "2018 DATE\n",
            "70% PERCENT\n",
            "10th ORDINAL\n",
            "2016 85% PERCENT\n",
            "Skills: []\n",
            "Education: []\n",
            "Experience (years): []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ranking Multiple Job Matches"
      ],
      "metadata": {
        "id": "2AGU1rcPw2_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Assume you already have resume_text and a list of job_descriptions\n",
        "job_descriptions = [\n",
        "    \"We need a Python developer with experience in Machine Learning and Data Analysis.\",\n",
        "    \"Looking for a Java backend engineer with SQL skills.\",\n",
        "    \"Data Scientist with Python, Deep Learning, and NLP experience.\",\n",
        "    \"Need a content writer\",\n",
        "    \"Looking for an expeirenced grapihc designer\",\n",
        "    \"Looking for a social media manager\",\n",
        "    \"looking for a junior data analyst\",\n",
        "]\n",
        "\n",
        "# Load the model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Encode the resume\n",
        "resume_embedding = model.encode(resume_text)\n",
        "\n",
        "# Encode all jobs\n",
        "job_embeddings = model.encode(job_descriptions)\n",
        "\n",
        "# Compute similarity\n",
        "similarity_scores = cosine_similarity([resume_embedding], job_embeddings)[0]\n",
        "\n",
        "# Rank them\n",
        "ranked_jobs = sorted(zip(job_descriptions, similarity_scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Top Job Matches:\")\n",
        "for idx, (job, score) in enumerate(ranked_jobs):\n",
        "    print(f\"{idx+1}. Score: {score:.4f} | Job: {job}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJtmZhvaw5ud",
        "outputId": "a458f6c8-7df3-4ea3-e58a-68205609310e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Job Matches:\n",
            "1. Score: 0.5172 | Job: looking for a junior data analyst\n",
            "2. Score: 0.4794 | Job: We need a Python developer with experience in Machine Learning and Data Analysis.\n",
            "3. Score: 0.4350 | Job: Data Scientist with Python, Deep Learning, and NLP experience.\n",
            "4. Score: 0.4272 | Job: Looking for a Java backend engineer with SQL skills.\n",
            "5. Score: 0.2937 | Job: Looking for an expeirenced grapihc designer\n",
            "6. Score: 0.2246 | Job: Looking for a social media manager\n",
            "7. Score: 0.2186 | Job: Need a content writer\n"
          ]
        }
      ]
    }
  ]
}